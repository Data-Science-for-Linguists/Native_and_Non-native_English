{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Katherine Kairis, kak275@pitt.edu, 10/12/2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import glob\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "transcripts = glob.glob('VOICE/VOICE1.0XML/XML/*.xml')\n",
    "del transcripts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create two dictionaries: one containing information about the participants, and one containing the conversations\n",
    "participants = {}\n",
    "conversations = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting info about the participants\n",
    "The participant_info function extracts information about the participants and stores it in the \"participants\" dictionary. The keys of the dictionary are the participants' ID numbers. The values are sub-dictionaries that include the participant's role, age, sex, and occupation (if listed). The sub-dictionaries also include the participants' L1s, which are stored in lists (since some participants have multiple L1s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def participant_info(contents):\n",
    "    \n",
    "    #Get all of the participants in the given conversation\n",
    "    people = contents.find('listPerson', {'type': 'identified'}).findAll('person')\n",
    "    \n",
    "    for p in people:\n",
    "        #info is a subdirector that contains a single participant's information. It will be \n",
    "        info = {}\n",
    "        info['role'] = p['role']\n",
    "        info['age'] = p.age.get_text()\n",
    "        info['sex'] = p.sex.get_text()\n",
    "        \n",
    "        #In some cases, the occupation isn't listed. If it is included, get the text of the occupation field.\n",
    "        #If it isn't included, \"None\" will be stored as the occupation, since p.occupation would return \"None.\"\n",
    "        try:\n",
    "            info['occupation'] = p.occupation.get_text()\n",
    "        except AttributeError:\n",
    "            info['occupation'] = p.occupation\n",
    "        \n",
    "        #Get a list of the languages that the participant speaks. Iterate through the list, and add them to the\n",
    "        #dictionary according to the speaker's level (ie. L1).\n",
    "        languages = p.findAll('langKnown')\n",
    "        for l in languages:\n",
    "            level = l['level']\n",
    "            language = l['tag']\n",
    "        \n",
    "            if level in info:\n",
    "                info[level].append(language)\n",
    "            else:\n",
    "                info[level] = [language]\n",
    "    \n",
    "        #Get the participant's ID number, and make it a key in the participants dictionary. The value will be\n",
    "        #the info dictionary\n",
    "        name = p['xml:id']\n",
    "        participants[name] = info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting lines of the conversation from the file\n",
    "The conversation_lines function gets each line from the current conversation. The lines are stored as lists in the \"conversations\" dictionary, whose keys are the names of the XML files. For now, I decided to keep the lines in their XML format; there are a lot of annotations in the XML format that could be useful later on, such as the speaker, pauses, and intonation markings. Converting the XML lines into text/getting rid of the tags is simple, so I could change this later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conversation_lines(file, contents):\n",
    "    file_name = file.split(\"/\")[-1]\n",
    "    text_body = contents.body\n",
    "    xml_lines = text_body.findAll('u')\n",
    "    conversations[file_name] = xml_lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing the XML files\n",
    "This section iterates through all of the files (except for corpus-header.xml) in the VOICE1.0XML/XML directory. It calls conversation_lines and participant_info to extract some important parts of the data from the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for t in transcripts:\n",
    "    file = open(t, 'r')\n",
    "    text = file.read()\n",
    "    xml_contents = BeautifulSoup(text, 'xml')\n",
    "    conversation_lines(t, xml_contents)\n",
    "    participant_info(xml_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<u who=\"#EDcon496_S1\" xml:id=\"EDcon496_u_1\"> e<c type=\"lengthening\"/>r leads so <pause/> ma<c type=\"lengthening\"/>n i'm still stuck on lead du<c type=\"lengthening\"/>de <pause dur=\"PT3S\"/></u>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversations['EDcon496.xml'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'L1': ['ger-AT', 'eng-US'],\n",
       " 'age': '25-34',\n",
       " 'occupation': None,\n",
       " 'role': 'participant',\n",
       " 'sex': 'female'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "participants['EDcon250_S2']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Size of corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of conversations: 151\n",
      "Average length of conversation: 698.8675496688742\n",
      "Number of participants: 1260\n"
     ]
    }
   ],
   "source": [
    "num_conversations = len(conversations)\n",
    "num_participants = len(participants)\n",
    "num_lines = 0\n",
    "\n",
    "for c in conversations:\n",
    "    num_lines += len(conversations[c])\n",
    "    \n",
    "print(\"Number of conversations:\", num_conversations)\n",
    "print(\"Average length of conversation:\", num_lines/num_conversations)\n",
    "print(\"Number of participants:\", num_participants)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of native English speakers\n",
    "I noticed that some of the participants in the conversations are native English speakers. Since I would like to separate native and non-native speakers in order to compare the two groups, I will have to remove these participants from this corpus. As of now, I'm not sure if I will include them in a native English corpus, or just completely discard them. Luckily, I won't have to delete too many participants from this corpus: only 87 of the 1260 participants have English listed as an L1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Get native English speakers\n",
    "native_speakers = []\n",
    "\n",
    "#There are multiple ways that English is listed as an L1 (\"eng\", \"eng-US\", \"eng-CA\", \"eng-GB\", \"eng-GY\", \"eng-AU\", etc)\n",
    "#I used a regular expression to find all of these instances\n",
    "r = re.compile(\"eng.*\")\n",
    "\n",
    "for person in participants:\n",
    "    \n",
    "    #returns a list of all languages that contain \"eng.*\" The length of this list should be 1 or 0. If it's 1, the\n",
    "    #participant has English listed as an L1.\n",
    "    english = list(filter(r.match, participants[person]['L1']))\n",
    "    \n",
    "    if len(english) != 0:\n",
    "        #print(person, ':', participants[person])\n",
    "        native_speakers.append(person)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of native speakers: 87\n",
      "Total number of participants: 1260\n",
      "\n",
      "Native English speakers:\n",
      "['EDcon250_S2', 'EDcon496_S2', 'EDcon521_S1', 'EDint328_S3', 'EDint330_S2', 'EDint330_S4', 'EDsed251_S3', 'EDsed301_S6', 'EDsed362_S1', 'EDsed362_S11', 'EDsed362_S14', 'EDsed362_S17', 'EDsed363_S3', 'EDsed364_S7', 'EDwgd497_S4', 'EDwgd5_S3', 'EDwgd6_S7', 'EDwgd6_S11', 'EDwsd15_S13', 'EDwsd242_S7', 'EDwsd302_S13', 'EDwsd303_S13', 'EDwsd304_S9', 'EDwsd306_S11', 'EDwsd590_S13', 'EDwsd9_S2', 'LEcon329_S4', 'LEcon545_S7', 'LEcon545_S1', 'LEcon547_S4', 'LEcon548_S4', 'LEcon548_S5', 'LEcon562_S2', 'LEcon562_S3', 'LEcon562_S6', 'PBmtg280_S3', 'PBmtg280_S4', 'PBpan10_S9', 'PBpan28_S7', 'PBpan28_S9', 'PBqas411_S1', 'PBqas412_S6', 'POcon549_S4', 'POcon591_S10', 'POmtg404_S5', 'POmtg439_S2', 'POmtg439_S3', 'POmtg444_S5', 'POmtg444_S9', 'POmtg447_S2', 'POmtg447_S3', 'POmtg546_S9', 'POprc522_S7', 'POprc522_S8', 'POprc558_S6', 'POprc559_S10', 'POprc559_S11', 'POwgd12_S1', 'POwgd12_S6', 'POwgd37_S10', 'POwgd375_S9', 'POwgd449_S6', 'POwgd449_S8', 'POwgd510_S8', 'POwsd256_S5', 'POwsd256_S6', 'POwsd257_S12', 'POwsd258_S8', 'POwsd266_S12', 'POwsd372_S12', 'POwsd374_S10', 'POwsd376_S5', 'POwsd376_S9', 'POwsd379_S10', 'PRcon536_S4', 'PRpan1_S3', 'PRpan13_S4', 'PRpan13_S5', 'PRpan13_S6', 'PRpan225_S4', 'PRpan225_S14', 'PRpan225_S15', 'PRqas19_S3', 'PRqas224_S2', 'PRqas495_S2', 'PRqas495_S5', 'PRqas495_S12']\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of native speakers:\", len(native_speakers))\n",
    "print(\"Total number of participants:\", len(participants))\n",
    "print(\"\\nNative English speakers:\")\n",
    "print(native_speakers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \"Unclear\" speech\n",
    "I also noticed that a lot of lines in the conversations had an \"unclear\" tag. I was thinking about removing these lines, but I was worried about possibly removing a large amount of the data. 9877 utterances out of the 105529 utterances had at least one \"unclear\" annotation, so I wouldn't lose a lot of data if I removed these lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_utterances = 0\n",
    "num_unclear = 0\n",
    "\n",
    "for c in conversations:\n",
    "    num_utterances += len(conversations[c])\n",
    "    for line in conversations[c]:\n",
    "        if line.findChildren('unclear'):\n",
    "            num_unclear += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of utterances: 105529\n",
      "Number of utterances with at least one \"unclear\" tag: 9877\n"
     ]
    }
   ],
   "source": [
    "print(\"Total number of utterances:\", num_utterances)\n",
    "print(\"Number of utterances with at least one \\\"unclear\\\" tag:\", num_unclear)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sharing data\n",
    "The license for the corpus seems very lenient, so I think that I could share as much data as I would like. The following comes from the corpus's license:\n",
    "* Attribution-NonCommercial-ShareAlike 3.0 Unported (CC BY-NC-SA 3.0)\n",
    "\n",
    "* You are free:\n",
    "    * to Share — to copy, distribute and transmit the work\n",
    "    * to Remix — to adapt the work\n",
    "\n",
    "* Under the following conditions:\n",
    "    * Attribution — You must attribute the work in the manner specified by the author or licensor (but not in any way that suggests that they endorse you or your use of the work).\n",
    "    * Noncommercial — You may not use this work for commercial purposes.\n",
    "    * Share Alike — If you alter, transform, or build upon this work, you may distribute the resulting work only under the same or similar license to this one."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
