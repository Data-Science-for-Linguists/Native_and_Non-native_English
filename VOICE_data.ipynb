{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Katherine Kairis, kak275@pitt.edu, 11/2/2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NEW CONTINUING -- This file continues upon the first progress report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import glob\n",
    "import re\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "transcripts = glob.glob('data/VOICE/VOICE2.0XML/XML/*.xml')\n",
    "del transcripts[0]\n",
    "\n",
    "tagged_transcripts = glob.glob('data/VOICE/VOICEPOSXML2.0/XML/*.xml')\n",
    "del transcripts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create three dictionaries\n",
    "participants = {}\n",
    "conversations = {}\n",
    "tagged_convs = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting info about the participants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def participant_info(contents):\n",
    "    \n",
    "    #Get all of the participants in the given conversation\n",
    "    people = contents.find('listPerson', {'type': 'identified'}).findAll('person')\n",
    "    \n",
    "    for p in people:\n",
    "        #info is a subdirector that contains a single participant's information. It will be \n",
    "        info = {}\n",
    "        info['role'] = p['role']\n",
    "        info['age'] = p.age.get_text()\n",
    "        info['sex'] = p.sex.get_text()\n",
    "        \n",
    "        #In some cases, the occupation isn't listed. If it is included, get the text of the occupation field.\n",
    "        #If it isn't included, \"None\" will be stored as the occupation, since p.occupation would return \"None.\"\n",
    "        try:\n",
    "            info['occupation'] = p.occupation.get_text()\n",
    "        except AttributeError:\n",
    "            info['occupation'] = p.occupation\n",
    "        \n",
    "        #Get a list of the languages that the participant speaks. Iterate through the list, and add them to the\n",
    "        #dictionary according to the speaker's level (ie. L1).\n",
    "        languages = p.findAll('langKnown')\n",
    "        for l in languages:\n",
    "            level = l['level']\n",
    "            language = l['tag']\n",
    "        \n",
    "            if level in info:\n",
    "                info[level].append(language)\n",
    "            else:\n",
    "                info[level] = [language]\n",
    "    \n",
    "        #Get the participant's ID number, and make it a key in the participants dictionary. The value will be\n",
    "        #the info dictionary\n",
    "        name = p['xml:id']\n",
    "        participants[name] = info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting lines of the conversation from the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conversation_lines(file, contents, li):\n",
    "    file_name = file.split(\"/\")[-1]\n",
    "    li[file_name] = contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conversations = {}\n",
    "for t in transcripts:\n",
    "    file = open(t, 'r')\n",
    "    text = file.read()\n",
    "    xml_contents = BeautifulSoup(text, 'xml')\n",
    "    conversation_lines(t, xml_contents, conversations)\n",
    "    participant_info(xml_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(conversations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Get native English speakers\n",
    "native_speakers = []\n",
    "\n",
    "#There are multiple ways that English is listed as an L1 (\"eng\", \"eng-US\", \"eng-CA\", \"eng-GB\", \"eng-GY\", \"eng-AU\", etc)\n",
    "#I used a regular expression to find all of these instances\n",
    "r = re.compile(\"eng.*\")\n",
    "\n",
    "for person in participants:\n",
    "    \n",
    "    #returns a list of all languages that contain \"eng.*\" The length of this list should be 1 or 0. If it's 1, the\n",
    "    #participant has English listed as an L1.\n",
    "    english = list(filter(r.match, participants[person]['L1']))\n",
    "    \n",
    "    if len(english) != 0:\n",
    "        #print(person, ':', participants[person])\n",
    "        native_speakers.append(person)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bilinguals = []\n",
    "L1_counts = {}\n",
    "#participants[native_speakers[0]]\n",
    "for p in participants:\n",
    "    #print(participants[p]['L1'])\n",
    "    languages = participants[p]['L1']\n",
    "    if len(languages) > 1:\n",
    "        bilinguals.append(p)\n",
    "        if p in native_speakers:\n",
    "            if 'eng' not in L1_counts:\n",
    "                L1_counts['eng'] = 1\n",
    "            else:\n",
    "                L1_counts['eng'] += 1\n",
    "        continue\n",
    "    \n",
    "    \n",
    "    for l in languages:\n",
    "        L1 = l.split(\"-\")[0]\n",
    "        if L1 not in L1_counts:\n",
    "            L1_counts[L1] = 1\n",
    "        else:\n",
    "            L1_counts[L1] += 1\n",
    "        #print(l.split(\"-\")[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "modified_conversations = {}\n",
    "lines = {}\n",
    "\n",
    "for file in conversations:\n",
    "    conv_lines = {}\n",
    "    \n",
    "    c = conversations[file].findAll('u')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Checks to make sure the line can be added to the dictionary.\n",
    "#A line must meet the following criteria: \n",
    "#the participant cannot be a native speaker of English\n",
    "#the participant must be listed in the participant directory\n",
    "#the participant cannot be bilingual\n",
    "#the line cannot contain any non-English words\n",
    "#the line cannot contain the speaker reading anything out loud\n",
    "def valid_utterance(participant, line):\n",
    "    if participant in native_speakers:\n",
    "        return False\n",
    "    if participant not in participants:\n",
    "        return False\n",
    "    #if len(text) == 0:\n",
    "    #    return False\n",
    "    if line.foreign != None:\n",
    "        return False\n",
    "    if line.unclear != None: \n",
    "        return False\n",
    "    if line.reading_aloud != None:\n",
    "        return False\n",
    "    if line.reading != None :\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Iterate through all of the files in VOICE to a nested dictionary that contains the word tokens of the\n",
    "#conversations.\n",
    "#The keys of the dictionaries are the file names. The values of these entries are subdictionaries. The keys of the\n",
    "#subdictionary are (participant, line_number) tuples, and the values are lists of tokens.\n",
    "tokenized_conversations = {}\n",
    "\n",
    "for file in conversations:\n",
    "    conv_lines = {}\n",
    "    \n",
    "    c = conversations[file].findAll('u')\n",
    "    \n",
    "    for l in c:\n",
    "        participant = l['who'].replace(\"#\", \"\")\n",
    "        line_id = l['xml:id']\n",
    "        text = l.get_text()\n",
    "        tokens = nltk.word_tokenize(text)\n",
    "        \n",
    "        if len(text) != 0 and valid_utterance(participant, l) == True:\n",
    "            key = (line_id, participant)\n",
    "            conv_lines[key] = tokens\n",
    "    \n",
    "    tokenized_conversations[file] = conv_lines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Get the text from the pos-tagged files\n",
    "tagged_conv_lines = {}\n",
    "for t in tagged_transcripts:\n",
    "    file = open(t, 'r')\n",
    "    text = file.read()\n",
    "    xml_contents = BeautifulSoup(text, 'xml')\n",
    "    conversation_lines(t, xml_contents, tagged_conv_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Iterate through all of the files in VOICE to a nested dictionary that contains the (word, tag) tuples from the\n",
    "#conversations.\n",
    "#The keys of the dictionaries are the file names. The values of these entries are subdictionaries. The keys of the\n",
    "#subdictionary are (participant, line_number) tuples, and the values are lists of (word, tag) tuples.\n",
    "tagged_conversations = {}\n",
    "\n",
    "for file in tagged_conv_lines:\n",
    "    conv_lines = {}\n",
    "    c = tagged_conv_lines[file].findAll('u')\n",
    "    \n",
    "    for l in c:\n",
    "        utterance = []\n",
    "        \n",
    "        participant = l['who'].replace(\"#\", \"\")\n",
    "        line_id = l['xml:id']\n",
    "        key = (participant, line_id)\n",
    "        \n",
    "        if valid_utterance(participant, l) == True:            \n",
    "            tags = l.findAll('w')\n",
    "            for t in tags:\n",
    "                word = t.text\n",
    "                ana = str(t).split()[1]\n",
    "                ana = ana.split(\"=\")\n",
    "                tag = ana[1][2:]\n",
    "                tag = tag.split('\"')[0]\n",
    "                #print(word, tag)\n",
    "            \n",
    "                utterance.append((word, tag))\n",
    "                \n",
    "            conv_lines[key] = utterance\n",
    "        \n",
    "    tagged_conversations[file] = conv_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Save the two dictionaries as pickle files\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = open('VOICE_tokenized.p', 'wb')\n",
    "pickle.dump(tokenized_conversations, f, -1)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = open('VOICE_tagged.p', 'wb')\n",
    "pickle.dump(tagged_conversations, f, -1)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
