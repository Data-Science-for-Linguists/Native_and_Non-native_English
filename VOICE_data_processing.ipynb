{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Katherine Kairis, kak275@pitt.edu, 10/12/2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import glob\n",
    "import re\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "transcripts = glob.glob('data/VOICE/VOICE2.0XML/XML/*.xml')\n",
    "del transcripts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create two dictionaries: one containing information about the participants, and one containing the conversations\n",
    "participants = {}\n",
    "conversations = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting info about the participants\n",
    "The participant_info function extracts information about the participants and stores it in the \"participants\" dictionary. The keys of the dictionary are the participants' ID numbers. The values are sub-dictionaries that include the participant's role, age, sex, and occupation (if listed). The sub-dictionaries also include the participants' L1s, which are stored in lists (since some participants have multiple L1s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def participant_info(contents):\n",
    "    \n",
    "    #Get all of the participants in the given conversation\n",
    "    people = contents.find('listPerson', {'type': 'identified'}).findAll('person')\n",
    "    \n",
    "    for p in people:\n",
    "        #info is a subdirector that contains a single participant's information. It will be \n",
    "        info = {}\n",
    "        info['role'] = p['role']\n",
    "        info['age'] = p.age.get_text()\n",
    "        info['sex'] = p.sex.get_text()\n",
    "        \n",
    "        #In some cases, the occupation isn't listed. If it is included, get the text of the occupation field.\n",
    "        #If it isn't included, \"None\" will be stored as the occupation, since p.occupation would return \"None.\"\n",
    "        try:\n",
    "            info['occupation'] = p.occupation.get_text()\n",
    "        except AttributeError:\n",
    "            info['occupation'] = p.occupation\n",
    "        \n",
    "        #Get a list of the languages that the participant speaks. Iterate through the list, and add them to the\n",
    "        #dictionary according to the speaker's level (ie. L1).\n",
    "        languages = p.findAll('langKnown')\n",
    "        for l in languages:\n",
    "            level = l['level']\n",
    "            language = l['tag']\n",
    "        \n",
    "            if level in info:\n",
    "                info[level].append(language)\n",
    "            else:\n",
    "                info[level] = [language]\n",
    "    \n",
    "        #Get the participant's ID number, and make it a key in the participants dictionary. The value will be\n",
    "        #the info dictionary\n",
    "        name = p['xml:id']\n",
    "        participants[name] = info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting lines of the conversation from the file\n",
    "The conversation_lines function gets each line from the current conversation. The lines are stored as lists in the \"conversations\" dictionary, whose keys are the names of the XML files. For now, I decided to keep the lines in their XML format; there are a lot of annotations in the XML format that could be useful later on, such as the speaker, pauses, and intonation markings. Converting the XML lines into text/getting rid of the tags is simple, so I could change this later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conversation_lines(file, contents):\n",
    "    file_name = file.split(\"/\")[-1]\n",
    "    text_body = contents.body\n",
    "    xml_lines = text_body.findAll('u')\n",
    "    conversations[file_name] = xml_lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing the XML files\n",
    "This section iterates through all of the files (except for corpus-header.xml) in the VOICE1.0XML/XML directory. It calls conversation_lines and participant_info to extract some important parts of the data from the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for t in transcripts:\n",
    "    file = open(t, 'r')\n",
    "    text = file.read()\n",
    "    xml_contents = BeautifulSoup(text, 'xml')\n",
    "    conversation_lines(t, xml_contents)\n",
    "    participant_info(xml_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<u who=\"#EDcon496_S1\" xml:id=\"EDcon496_u_1\"> e<c type=\"lengthening\"/>r leads so <pause/> ma<c type=\"lengthening\"/>n i'm still stuck on lead du<c type=\"lengthening\"/>de <pause dur=\"PT3S\"/></u>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversations['EDcon496.xml'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'L1': ['ger-AT', 'eng-US'],\n",
       " 'age': '25-34',\n",
       " 'occupation': None,\n",
       " 'role': 'participant',\n",
       " 'sex': 'female'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "participants['EDcon250_S2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Get native English speakers\n",
    "native_speakers = []\n",
    "\n",
    "#There are multiple ways that English is listed as an L1 (\"eng\", \"eng-US\", \"eng-CA\", \"eng-GB\", \"eng-GY\", \"eng-AU\", etc)\n",
    "#I used a regular expression to find all of these instances\n",
    "r = re.compile(\"eng.*\")\n",
    "\n",
    "for person in participants:\n",
    "    \n",
    "    #returns a list of all languages that contain \"eng.*\" The length of this list should be 1 or 0. If it's 1, the\n",
    "    #participant has English listed as an L1.\n",
    "    english = list(filter(r.match, participants[person]['L1']))\n",
    "    \n",
    "    if len(english) != 0:\n",
    "        #print(person, ':', participants[person])\n",
    "        native_speakers.append(person)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "participant = native_speakers[0]\n",
    "languages = participants[participant]['L1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "english_dialects = {}\n",
    "for p in native_speakers:\n",
    "    languages = participants[p]['L1']\n",
    "    for l in languages:\n",
    "        if 'eng' not in l:\n",
    "            continue\n",
    "        #print(l)\n",
    "        \n",
    "        if l not in english_dialects:\n",
    "            english_dialects[l] = 1\n",
    "        else:\n",
    "            english_dialects[l] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eng': 8,\n",
       " 'eng-AU': 3,\n",
       " 'eng-CA': 6,\n",
       " 'eng-GB': 39,\n",
       " 'eng-GY': 1,\n",
       " 'eng-IE': 3,\n",
       " 'eng-MT': 7,\n",
       " 'eng-US': 20}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_dialects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bilinguals = []\n",
    "L1_counts = {}\n",
    "#participants[native_speakers[0]]\n",
    "for p in participants:\n",
    "    #print(participants[p]['L1'])\n",
    "    languages = participants[p]['L1']\n",
    "    if len(languages) > 1:\n",
    "        bilinguals.append(p)\n",
    "        if p in native_speakers:\n",
    "            if 'eng' not in L1_counts:\n",
    "                L1_counts['eng'] = 1\n",
    "            else:\n",
    "                L1_counts['eng'] += 1\n",
    "        continue\n",
    "    \n",
    "    \n",
    "    for l in languages:\n",
    "        L1 = l.split(\"-\")[0]\n",
    "        if L1 not in L1_counts:\n",
    "            L1_counts[L1] = 1\n",
    "        else:\n",
    "            L1_counts[L1] += 1\n",
    "        #print(l.split(\"-\")[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bilinguals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ger-AT', 'eng-US']\n",
      "['eng-GY', 'dut-NL']\n",
      "['fre-FR', 'eng']\n",
      "['mlt-MT', 'eng-MT']\n",
      "['mlt-MT', 'eng-MT']\n",
      "['mlt-MT', 'eng-MT']\n",
      "['mlt-MT', 'ger-AT']\n",
      "['ger-AT', 'eng-US']\n",
      "['ukr-UA', 'rus']\n",
      "['ger-DE', 'ind-ID']\n",
      "['ger-AT', 'eng-US']\n",
      "['eng', 'heb', 'dut']\n",
      "['slo-SK', 'ger']\n",
      "['eng-CA', 'chi-CN']\n",
      "['eng-CA', 'chi-CN']\n",
      "['fre-CH', 'ger-CH']\n",
      "['dut-NL', 'eng']\n",
      "['eng-GB', 'spa-ES']\n",
      "['fre-CH', 'ger-CH']\n",
      "['eng-GB', 'spa-ES']\n",
      "['eng-GB', 'spa-ES']\n",
      "['eng-GB', 'spa-ES']\n",
      "['fre-CH', 'ger-CH']\n",
      "['eng-GB', 'spa-ES']\n",
      "['eng-GB', 'spa-ES']\n",
      "['fre-CH', 'ger-CH']\n",
      "['eng-GB', 'spa-ES']\n",
      "['fre-CH', 'ger-CH']\n",
      "['eng-GB', 'spa-ES']\n",
      "['mlt-MT', 'eng-MT']\n",
      "['cat-ES', 'spa-ES']\n",
      "['mlt-MT', 'eng-MT']\n",
      "['mlt-MT', 'eng-MT']\n",
      "['mlt-MT', 'eng-MT']\n",
      "['spa-ES', 'cat-ES']\n",
      "['dut', 'ger-AT']\n",
      "['spa-ES', 'cat']\n",
      "['por-PT', 'ger-AT']\n",
      "['por-PT', 'ger-AT']\n",
      "['dut', 'ger-AT']\n",
      "['ger', 'ita']\n",
      "['ger-AT', 'pol-PL']\n",
      "['ara-PS', 'ger-AT']\n",
      "['per-IR', 'eng-US']\n"
     ]
    }
   ],
   "source": [
    "for p in bilinguals:\n",
    "    print(participants[p]['L1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alb': 11,\n",
       " 'ara': 4,\n",
       " 'arm': 6,\n",
       " 'bos': 5,\n",
       " 'bul': 13,\n",
       " 'cat': 6,\n",
       " 'chi': 7,\n",
       " 'cze': 18,\n",
       " 'dan': 35,\n",
       " 'dut': 72,\n",
       " 'eng': 87,\n",
       " 'est': 8,\n",
       " 'fin': 51,\n",
       " 'fre': 63,\n",
       " 'ger': 303,\n",
       " 'gre': 14,\n",
       " 'hin': 4,\n",
       " 'hun': 13,\n",
       " 'ice': 3,\n",
       " 'ind': 2,\n",
       " 'ita': 54,\n",
       " 'jpn': 5,\n",
       " 'kaz': 2,\n",
       " 'kir': 2,\n",
       " 'kor': 14,\n",
       " 'lav': 19,\n",
       " 'lit': 9,\n",
       " 'mac': 14,\n",
       " 'mlt': 22,\n",
       " 'nor': 34,\n",
       " 'per': 2,\n",
       " 'pol': 35,\n",
       " 'por': 21,\n",
       " 'rum': 30,\n",
       " 'rus': 22,\n",
       " 'scc': 24,\n",
       " 'scr': 13,\n",
       " 'slo': 29,\n",
       " 'slv': 16,\n",
       " 'spa': 72,\n",
       " 'swe': 16,\n",
       " 'tgl': 1,\n",
       " 'tur': 15,\n",
       " 'ukr': 4,\n",
       " 'und': 37,\n",
       " 'urd': 2,\n",
       " 'vie': 1,\n",
       " 'yor': 1}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L1_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "modified_conversations = {}\n",
    "lines = {}\n",
    "\n",
    "for file in conversations:\n",
    "    conv_lines = {}\n",
    "    \n",
    "    c = conversations[file]\n",
    "    \n",
    "    for l in c:\n",
    "        participant = l['who'].replace(\"#\", \"\")\n",
    "        line_id = l['xml:id']\n",
    "        text = l.get_text()\n",
    "        tokens = nltk.word_tokenize(text)\n",
    "        \n",
    "        if participant in native_speakers:\n",
    "            continue\n",
    "            \n",
    "        if participant not in participants:\n",
    "            #print(participant)\n",
    "            continue\n",
    "        \n",
    "        elif len(text) == 0:\n",
    "            continue\n",
    "    \n",
    "        elif l.foreign != None:     #returns the line if has \"foreign\" tag; None if it doesn't contain the tag\n",
    "            #print(l.foreign.get_text())   #text of line  \n",
    "            #print(l.foreign['xml:lang'])  #language used\n",
    "            #print(l)                      #line (in XML format)\n",
    "            continue\n",
    "        \n",
    "        elif l.unclear != None:     #returns the line if has \"unclear\" tag; None if it doesn't contain the tag\n",
    "            #    print(l)\n",
    "            continue\n",
    "    \n",
    "        elif l.reading_aloud != None:\n",
    "            continue\n",
    "            #print(l)\n",
    "        \n",
    "        elif l.reading != None :\n",
    "            continue\n",
    "            #print(l)\n",
    "        \n",
    "        else:\n",
    "            key = (line_id, participant)\n",
    "            #conv_lines[key] = text\n",
    "            conv_lines[key] = tokens\n",
    "    \n",
    "    modified_conversations[file] = conv_lines\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pprint\n",
    "#modified_conversations['EDcon4.xml']\n",
    "#modified_conversations['EDsed364.xml']\n",
    "#modified_conversations['EDsed362.xml']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##Processing Part of speech tagged corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "transcripts = glob.glob('data/VOICE/VOICEPOSXML2.0/XML/*.xml')\n",
    "del transcripts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversations = {}\n",
    "for t in transcripts:\n",
    "    file = open(t, 'r')\n",
    "    text = file.read()\n",
    "    xml_contents = BeautifulSoup(text, 'xml')\n",
    "    #conversation_lines(t, xml_contents)\n",
    "    conversations[file] = xml_contents\n",
    "    #participant_info(xml_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "utterances = xml_contents.body.findAll('u')\n",
    "\n",
    "for u in utterances:\n",
    "    participant = u['who'].replace(\"#\", \"\")\n",
    "    line_id = u['xml:id']\n",
    "    \n",
    "    key = (participant, line_id)\n",
    "    #print(key)\n",
    "    \n",
    "    for t in tags:\n",
    "        word = t.get_text()\n",
    "        tag = t['ana'].replace(\"#\", \"\")\n",
    "        tu = (word, tag)\n",
    "        #print('\\t', tu)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'EDcon4.xml'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-109-71c44c795780>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mconversations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'EDcon4.xml'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m: 'EDcon4.xml'"
     ]
    }
   ],
   "source": [
    "conversations['EDcon4.xml']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
