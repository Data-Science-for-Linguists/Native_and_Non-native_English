# Report 1 (Project Plan)
## 10/3/2017
I found a corpus of English conversations between nonnative speakers, called the Vienna-Oxford International Corpus of English. My next steps will include
1. Finding a corpus of conversations between native English speakers.
2. Beginning to process the corpus/read the files. This step could take some time, since the Vienna-Oxford International Corpus contains markdown files. I will also probably have to reformat the data.

# Report 2 (First Progress Report)
## 10/12/2017
I used BeautifulSoup to process the XML files in the Vienna-Oxford corpus. Reading and manipulating the data didn't take as much time or work as I originally expected, but it did take some time to get used to the BeautifulSoup commands. I think that, as of now, my organization of the data is pretty good, but I am expecting to modify it later on as my goals become more clear.
Even though the Vienna-Oxford corpus focuses on non-native speakers, some of the participants listed in the corpus do speak English as a native language. One of the statistics that I found was the number of participants who have English listed as an L1. While the proportion of native English speakers was small, there were more than I expected. Originally, I was planning on finding an additional corpus of native English to compare to the Vienna-Oxford corpus; however, now I'm considering removing all of the native English speakers from the Vienna-Oxford corpus, and creating a new corpus from their speech. I think that the main advantage of creating this new corpus is consistency. To compare native and non-native speakers, I would have to find another corpus with a similar annotation system, which could be difficult. If I create this new corpus, the native speech and nonnative speech would come from the same corpus and have the same annotations, making the comparisons much more feasible. However, since the nonnative speakers vastly outnumber the native speakers, the native corpus would be much smaller than the nonnative corpus, which isn't ideal.

# Report 3
## 10/31/2017
I looked into the distribution of dialects among the native speakers in the VOICE corpus, and the majority spoke British English. I downloaded the British National Corpus, and I plan on comparing it to the entries in the Vienna-Oxford Corpus of International English. About 90% of the files in the BNC are from written sources, and the other 10% are transcripts of speech. I had to spend a decent amount of time writing code to show which files are speech and which are written. The speech transcripts contain the tag <stext>, which encases all of the speech in the file. The written files do not have this tag. To distinguish between the two types, I check each file for the tag. After I determined which file came from written sources, I deleted them since I won't need them and they took up a lot of space. Each word in the BNC is tagged with it part of speech tag and lemma. I also noticed that the VOICE corpus has a part-of-speech tagged file, so part of speech tags could be interestng to use.

# Report 4
## 11/2/2017 (Second Progress Report)
I created two files to process the two corpora, and I think that I finalized my data organization. The corpora are organized in nested dictionaries, and each corpora has two dictionaries: one for tokenized words and the other for lists of (word, part-of-speech tag) tuples. I'm still not completely comfortable with BeautifulSoup, and I did run into issues with it, so this step took much longer than I expected it to. Even though I think that my data organization is practically finalized, I may go back and modify the data later on. For example, the two corpora use different tag sets, which isn't very useful if I want to compare them. So I could look into the two tag sets more closely and try to convert the tags of one (or both) of the corpora so they match. Another thing I was think of looking into was whether the participants' speech changed when they were conversing with native speakers. To do this, I would probably have to create a dictionary that has each conversation to the total participants and the number of participants who are native speakers. When I finished these steps, I performed some very basic analyses on the corpora. I looked at the average utterance length, common words, bigrams, and stop words. While there were a few things that caught my attention from these analyses, I didn't notice any major differences between the two corpora. In addition to looking at other features, I plan on further investigating words and bigrams, since I think that there probably are significant differences that aren't clear at first glance. I also plan on looking into differences in L1 groups in the VOICE corpus, and I think that bigrams could be very useful here.